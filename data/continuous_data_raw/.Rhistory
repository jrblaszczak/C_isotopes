## Import packages
lapply(c("plyr","dplyr","ggplot2","cowplot",
"lubridate","tidyverse"), require, character.only=T)
## Import packages
lapply(c("plyr","dplyr","ggplot2","cowplot",
"lubridate","tidyverse"), require, character.only=T)
# Iteratively import
dat <- read.csv("../data/discharge_data_raw/2018_09_28_Cond_Pink_LolomaiOakCreek.csv", skip=1, header=T)
View(dat)
names(dat)
# Subset
dat <- dat[,c(2:4)]
colnames(dat) <- c("DateTime","Cond","TempC")
# Convert DateTime
dat$DateTime <- as.POSIXct(as.character(dat$DateTime), format="%m/%d/%y %H:%M:%S")
head(dat)
sapply(dat, class)
# Visualize
ggplot(dat, aes(DateTime, Cond))+geom_point()
## Subset based on plot
dat <- subset(dat, DateTime >= as.POSIXct("2018-09-28 07:15:00") & DateTime <= as.POSIXct("2018-09-28 09:00:00")) # Lolomai
################
## Estimate Q ##
################
## Equation
Qint<-function(time,cond, bkg, condmass){
condcorr<-cond-bkg
##below routine integrates
ydiff<- condcorr[-1]+ condcorr[-length(condcorr)]
condint<-sum(diff(time)*ydiff/2)
Q<-condmass/condint
Q
}
sub_bg <- subset(dat, DateTime >= as.POSIXct("2018-09-28 08:30:00") & DateTime <= as.POSIXct("2018-09-28 09:00:00")) #Lolomai
bg_cond <- mean(sub_bg$Cond)
Cond_mass <- 2100*3000
Oak_Lolomai_Clmass <- 3000 # Oak @ Lolomai: 3.00 kg
Oak_Willow_Clmass <- 7210 # Oak @ Willow: 7.21 kg
Blaine_Clmass <- 1209 # Blaine: 1209 g
Beaver_Clmass <- 2084 # Beaver: 2084 g
Cond_mass <- 2100*Oak_Lolomai_Clmass ## *replace depending on calculation
## Calculate Q!
## Units = L/sec
Q <- Qint(as.numeric(dat$DateTime), dat$Cond, bg_cond, Cond_mass)
Q
inj_time <- as.POSIXct("2018-09-28 07:23:00") #Lolomai
peak_time <- dat[which.max(dat$Cond),]$DateTime
time_diff_sec <- as.numeric(peak_time - inj_time)*60
v <- 107.5/time_diff_sec
v
## Distance upstream
Oak_Lolomai_dist_upstream <- 107.5 # Oak @ Lolomai: 107.5 m upstream
Oak_Willow_dist_upstream <- 150 # Oak @ Willow: 150 m upstream
Blaine_dist_upstream <- 50 # Blaine: 50 m upstream
Beaver_dist_upstream <- 217 # Beaver: 217 m upstream
v <- Oak_Lolomai_dist_upstream/time_diff_sec ## *replace depending on calculation
v
w <- 8.6
## Calculate effective depth
z <- (Q/1000)/(w*v)
z
##==============================================================================
## Project: CO2 isotopes
## Script to process continuous sensor data from diel sampling
## Code author: J.R. Blaszczak
##==============================================================================
## Load packages
lapply(c("plyr","dplyr","ggplot2","cowplot",
"lubridate","tidyverse", "readxl"), require, character.only=T)
setwd("../data/continuous_data_raw")
## Import DO data
DOdat <- ldply(list.files(pattern = "*_DOcompiled.csv"), function(filename) {
d <- read.csv(filename)
d$file <- filename
return(d)
})
names(DOdat)
DOdat <- DOdat[,c("Time","Temp","DO","file")]
colnames(DOdat) <- c("DateTime","temp","oxy","file")
sDO <- split(DOdat, DOdat$file)
names(sDO) <- c("Blaine","Beaver","OakLolomai","OakWillow")
sDO <- lapply(sDO, function(x) return(x[,c("DateTime","temp","oxy")]))
dDO <- ldply(sDO, data.frame)
dDO$DateTime <- as.POSIXct(as.character(dDO$DateTime), format = "%Y-%m-%d %H:%M:%S")
## Round DateTime to the nearest 5 minute interval to help with integration
dDO$DateTime <- floor_date(dDO$DateTime, "5 minutes")
## Visualize
ggplot(dDO, aes(DateTime, oxy))+geom_line()+
facet_wrap(~.id)
## Save compiled file
write.csv(dDO, "../data/processed_data/DO_continuous_data.csv")
getwd()
## Save compiled file
write.csv(dDO, "../processed_data/DO_continuous_data.csv")
